{
 "metadata": {
  "name": "",
  "signature": "sha256:cc31fac0ca28516f6d4c1f39e55e151bcb9a9c2e419c4568dc84cdd1d7c1e44e"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import numpy as np\n",
      "\n",
      "def forest_management(n=3, p=0.1, r1=4, r2=2):\n",
      "    P = np.zeros((n, n, 2))\n",
      "    P[:,:,0] = (1. - p)*np.diag(np.ones(n-1), 1)\n",
      "    P[:,0,0] = p\n",
      "    P[-1,-1,0] = 1. - p\n",
      "    P[:,0,1] = 1.\n",
      "    \n",
      "    r = np.zeros((n, 2))\n",
      "    r[-1,0] = r1\n",
      "    r[1:,1] = 1.\n",
      "    r[-1,1] = r2\n",
      "\n",
      "    return P, r"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 1
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def value_iteration(P, r, gamma, epsilon, verbose=False):\n",
      "    n, m = P.shape[0], P.shape[2]\n",
      "    v = np.zeros(n)\n",
      "    \n",
      "    stopping = epsilon*(1. - gamma)/gamma\n",
      "    span_norm = lambda v: np.max(v) - np.min(v)\n",
      "    def bellman_update(v):\n",
      "        Q = np.zeros((n,m))\n",
      "        for a in range(m):\n",
      "            Q[:, a] = r[:, a] + gamma*P[:, :, a].dot(v)\n",
      "        return (Q.max(axis=1), v, Q.argmax(axis=1))\n",
      "    \n",
      "    v, vprev, policy = bellman_update(v)\n",
      " \n",
      "    i = 0\n",
      "    while span_norm(v - vprev) > stopping:\n",
      "        if verbose: print 'Iteration {0}: {1}'.format(i, span_norm(v - vprev))\n",
      "        v, vprev, policy = bellman_update(v)\n",
      "        i += 1\n",
      "        \n",
      "    return v, policy"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 2
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def policy_evaluation_inv(P, r, gamma, policy):\n",
      "    n = P.shape[0]\n",
      "    Q = np.zeros((n,n))\n",
      "    for s in range(n):\n",
      "        Q[s,:] = P[s,:,policy[s]]\n",
      "        \n",
      "    R = np.zeros(n)\n",
      "    for s in range(n):\n",
      "        R[s] = r[s,policy[s]]\n",
      "        \n",
      "    return np.linalg.solve(np.eye(Q.shape[1]) - gamma*Q, R)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 3
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def policy_evaluation_iter(P, r, gamma, policy):\n",
      "    n = P.shape[0]\n",
      "    v = np.zeros(n)\n",
      "\n",
      "    Q = np.zeros((n,n))\n",
      "    for s in range(n):\n",
      "        Q[s,:] = P[s,:,policy[s]]\n",
      "    Q = gamma*Q\n",
      "        \n",
      "    R = np.zeros(n)\n",
      "    for s in range(n):\n",
      "        R[s] = r[s,policy[s]]\n",
      "        \n",
      "    stopping = epsilon*(1. - gamma)/gamma\n",
      "    span_norm = lambda v: np.max(v) - np.min(v)\n",
      "    bellman_update = lambda v: (R + Q.dot(v), v)\n",
      "\n",
      "    v, vprev = bellman_update(v)\n",
      "    while span_norm(v - vprev) > stopping:\n",
      "        v, vprev = bellman_update(v)\n",
      "        \n",
      "    return v"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 4
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def policy_iteration(P, r, gamma, verbose=False):\n",
      "    n = P.shape[0]\n",
      "    m = P.shape[2]\n",
      "    v = np.zeros(n)\n",
      "    policy = np.zeros(n)\n",
      "    policy_prev = np.ones(n)\n",
      "\n",
      "    def policy_improvement(v):\n",
      "        Q = np.zeros((n,m))\n",
      "        for a in range(m):\n",
      "            Q[:,a] = r[:,a] + gamma*P[:,:,a].dot(v)\n",
      "        return (np.argmax(Q, axis=1), policy)\n",
      "    \n",
      "    i = 0\n",
      "    while np.any(policy != policy_prev): \n",
      "        v = policy_evaluation_inv(P, r, gamma, policy)\n",
      "        policy, policy_prev = policy_improvement(v)\n",
      "        if verbose: print 'Iteration {0}'.format(i)\n",
      "        i += 1\n",
      "        \n",
      "    return policy"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 5
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "gamma, epsilon = 0.2, 0.01\n",
      "P, r = forest_management()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 6
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print 'Policy eval. by inverse',  policy_evaluation_inv(P, r, gamma, [0,1,0])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Policy eval. by inverse [ 0.19067797  1.03813559  4.88269946]\n"
       ]
      }
     ],
     "prompt_number": 7
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "v, policy_vi = value_iteration(P, r, gamma, epsilon)\n",
      "policy_pi = policy_iteration(P, r, gamma)\n",
      "print 'Optimal policies are equal ? {0}'.format(not np.any(policy_vi != policy_pi))\n",
      "print 'Optimal policy:', policy_pi\n",
      "print 'Policy eval. by inverse',  policy_evaluation_inv(P, r, gamma, [0,1,0])\n",
      "print 'Policy eval. by fixed point', policy_evaluation_iter(P, r, gamma, [0,1,0])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Optimal policies are equal ? True\n",
        "Optimal policy: [0 1 0]\n",
        "Policy eval. by inverse [ 0.19067797  1.03813559  4.88269946]\n",
        "Policy eval. by fixed point [ 0.190152  1.03672   4.877248]\n"
       ]
      }
     ],
     "prompt_number": 8
    }
   ],
   "metadata": {}
  }
 ]
}