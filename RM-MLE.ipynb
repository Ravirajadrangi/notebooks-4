{
 "metadata": {
  "name": "",
  "signature": "sha256:5c14354a1186fdd57ad235e474294905fd12af669f171697ed013574bb36dc7d"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# The origins of SGD in Robbins-Monro \n",
      "\n",
      "In the following, we cast our classification problem under the empirical risk minimization framework. More precisely, we will consider the case where the error function is a decomposable as a sum of individual error contributions from each training pattern:\n",
      "\\begin{equation}\n",
      "\\sum_{i} E(\\mathbf{e}_i^\\top \\mathbf{X}, t_i)\n",
      "\\end{equation}\n",
      "\n",
      "where $E$ is a per-instance error function and where the matrix $\\mathbf{X}$ consists of the training instances as rows and has as many columns  as the dimensionality of the input vectors. While the sum-of-squares error function is commonplace in the regression setting, we will rather consider the negative log-likelihood criterion for our classification task:\n",
      "\\begin{equation}\n",
      "E(\\mathbf{x}, t) = -\\log(y_t)\n",
      "\\end{equation}\n",
      "\n",
      "In this expression, we used $y_t$ to denote activation of the output neuron of the true class $t$.Under the probabilistic interpretation obtained through the softmax transformation, we can think of the above cost function as being equivalent to the cross-entropy measure. From a coding perspective, the cross-entropy measures the number of bits necessary when some other distribution is used instead of the true one. \n",
      "\n",
      "Because of the non-linearities introduced in our model, we can no longer solve the empirical risk minimization problem either in closed-form or through standard convex optimization methods. We will then have recourse to the stochastic approximation framework to update the parameters (the weights and biases) of our model in an incremental fashion. The Robbins-Monro (RM) algorithm (see \\cite{Bishop1995} section 2.4.1) was initially introduced for the problem of root-finding for stochastic functions. That is, we assume that samples of pairs (in the unidimensional setting) of correlated variables are available from which one can compute their empirical averages as a proxy to their expectation. A stochastic function $f$ is defined in terms of the conditional expectation of $X$ given $Y$:\n",
      "\\begin{equation*}\n",
      "f(Y) = \\mathbb{E}\\left\\{ X | Y\\right\\}\n",
      "\\end{equation*}\n",
      "\n",
      "The RM algorithms then solves for the values $X^*$ such that \n",
      "\\begin{equation*}\n",
      "f(X^*) = 0\n",
      "\\end{equation*}\n",
      "\n",
      "A sequence of updates of the following form can be shown to converge to $X^*$:\n",
      "\\begin{equation*}\n",
      "X_{t+1} = X_t + \\alpha_t X(Y)\n",
      "\\end{equation*}\n",
      "\n",
      "where $X(Y)$ stands for the observed value of $X$ when $Y$ occurs. The $\\alpha$ coefficients, which we sometimes refer to as the *learning rate*, must obey the following conditions in order for the RM algorithm to converge:\n",
      "\\begin{align}\n",
      "\\lim_{t \\to \\infty} \\alpha_t = 0 \\hspace{5mm} \\sum_{t=1}^\\infty a_t = \\infty \\hspace{5mm} \\sum_{t=1}^\\infty a^2_t < \\infty\n",
      "\\end{align}\n",
      "\n",
      "A remarkable consequence of the existence of the RM algorithm is that we can now use it to solve maximum likelihood parameter estimates (MLE). That is, the MLE $\\hat{\\theta}$ of the parameters of some probabilistic model is found by solving for \n",
      "\\begin{equation}\n",
      "\\frac{\\partial}{\\partial\\theta}\\left( \\prod_{n=1}^N \\mathbb{P}\\left\\{x_n | \\theta \\right\\} \\right) = 0\n",
      "\\end{equation}\n",
      "\n",
      "We apply the usual trick of transforming the above product into a sum by taking the log. Taking the limit of the log-transformed derive, we would find that:\n",
      "\\begin{equation}\n",
      "\\mathbb{E}\\left\\{ \\frac{\\partial}{\\partial \\theta} \\ln \\mathbb{P} \\left\\{ x | \\theta \\right\\}  \\right\\}  =  0\n",
      "\\end{equation}\n",
      "\n",
      "The above is the asymptotical equivalent of the maximum likelihood estimate. The RM algorithm would then apply readily and compute the following updates:\n",
      "\\begin{equation}\n",
      "\\theta_{t+1} = \\theta_t + \\alpha_t \\frac{\\partial}{\\partial \\theta} \\ln \\mathbb{P}\\left\\{ x_{t+1} | \\theta \\right\\} \n",
      "\\end{equation}\n",
      "\n",
      "Under the negative log-likehood criterion, we can therefore see how the RM algorithm can be used to find parameters for an artificial neural network. In order to do so, we will however have to find the derivative of the loss function with respect to the parameters. This will be achieved through the backpropagation algorithm. \n",
      "\n",
      "## Reference\n",
      "\n",
      "For a better explanation, take a look at Bishop 1995 section 2.4.1"
     ]
    }
   ],
   "metadata": {}
  }
 ]
}