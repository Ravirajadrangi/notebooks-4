{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 align=center>Fall 2015: An intro to <a href=\"http://cs.mcgill.ca/~jpineau/comp598/\">Applied Machine Learning</a> in Python</h1>\n",
    "<h2 align=center>Pierre-Luc Bacon</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to Python at McGill\n",
    "\n",
    "Python is great because it has so many packages. However, you will quickly discover that installing packages without administrative privileges can become challenging. There is [virtualenv](http://www.virtualenv.org/en/latest/) that lets you install packages locally but it can be a nightmare when you have C librairies to build. \n",
    "\n",
    "If you want to do scientific computing with Python, I advise you to go with the [Anaconda](http://continuum.io/downloads) Python distribution. It will save you (and the SOCS sysadmins) a great deal of time. To install Anaconda on Linux for 64 bits machines, it would look like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "wget http://repo.continuum.io/archive/Anaconda3-2.3.0-Linux-x86_64.sh\n",
    "chmod +x Anaconda3-2.3.0-Linux-x86_64.sh\n",
    "./Anaconda3-2.3.0-Linux-x86_64.sh\n",
    "conda update conda\n",
    "conda update ipython"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "IPython notebook can then be started from the command line with `ipython notebook` in a directory where you want to store your notebooks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The magic of IPython\n",
    "\n",
    "If you miss the interactive style of the Matlab prompt, it's a good idea to use the [IPython](http://ipython.org/) shell or the web-based [IPython notebook](http://ipython.org/notebook.html). Notebooks embody the idea of [literate programming](http://en.wikipedia.org/wiki/Literate_programming) interleaving text, math rendering, executable code, plots and other forms of medias. It's also a great way to embrace reproducibility ! Download this notebook and see for yourself.  \n",
    "\n",
    "Although IPython has the *python* suffix in its name, its modular design allows for a great deal of interoperability with other languages. For example, you can call bash through the so-called *cell magics*. For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to capture the output of a bash cell into Python variables, you could also do:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%bash --out output --err error\n",
    "echo $RANDOM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another useful cell magic is also `%timeit`, giving you an estimate of the time spent executing a given function. For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%timeit np.linalg.eigvals(np.random.rand(50,50))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can learn more about various cell magics in this [notebook](http://nbviewer.ipython.org/github/ipython/ipython/blob/1.x/examples/notebooks/Cell%20Magics.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Numpy basics\n",
    "\n",
    "Numpy is a Python library which provides matrix manipulation capabilities and other standard numerical algorithms. \n",
    "\n",
    "Numpy and smart vectorization is the key to fast scientific computing in Python. In fact, it's the only advice that I strongly recommend following:\n",
    "\n",
    "<span style=\"color:red\">Avoid for loops at all cost !</span>\n",
    "\n",
    "Aside from that, the nice thing about learning Python is that ... you don't need to learn Python. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating arrays\n",
    "\n",
    "You can create a vector from an existing tuple:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x = np.array([1,2,3,4])\n",
    "print(x)\n",
    "print(x.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarly, you can define a numpy matrix as:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X = np.array([[1,2], [3,4]])\n",
    "print(X)\n",
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also easily generate points on an interval through [linspace](http://docs.scipy.org/doc/numpy/reference/generated/numpy.linspace.html) or [logspace](http://docs.scipy.org/doc/numpy/reference/generated/numpy.logspace.html):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x = np.linspace(0, 10, 25)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The standards matrices can be generated with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "np.zeros((5,5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "np.ones((5,5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "np.eye(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Indexing\n",
    "\n",
    "As opposed to Matlab, Numpy is 0-indexed. Indexing can be performed efficiently through *slicing*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X = np.random.randn(4,4)\n",
    "print(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Selecting all rows except the first and select all columns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X[1:,:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Select columns 2 and 0 and all rows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X[:,[2,0]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`-1` (minus one) stands for the last element in a given dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X[-1,:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A powerful indexing feature can also be achieved through *logical indexing*. You can create a boolean array from an ndarray:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X < 0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The boolean ndarray can then be used for indexing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X[X < 0.5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Avoiding for-loops: broadcasting\n",
    "\n",
    "Thinking about for-loops or Matlab's [repmat](http://www.mathworks.com/help/matlab/ref/repmat.html) ? The chances are that you can avoid all of that and leverage the powerful concept of [broadcasting](http://docs.scipy.org/doc/numpy/user/basics.broadcasting.html) in Numpy. \n",
    "\n",
    "Broadcasting allows you to essentially mimic the process of copying or repeating a given array object to match the dimensions of a larger array. Some magics is implemented under the hood (within [Blas](https://en.wikipedia.org/wiki/Basic_Linear_Algebra_Subprograms)) which makes broadcasting operations both cpu and memory efficient."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://www.astroml.org/_images/fig_broadcast_visual_1.png\"/>\n",
    "\n",
    "(Figure taken from [AstroML](http://www.astroml.org/book_figures/appendix/fig_broadcast_visual.html))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The simplest form of broadcasting might be seen in the matrix or vector-scalar operation of addition:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "np.arange(4) + 0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which is just equivalent to:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "np.arange(4) + np.array([0.5, 0.5, 0.5, 0.5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "but without the memory overhead. We can also broadcast higher order arrays onto each other. For example, a vector-matrix broadcasting on the columns:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "A = np.arange(6).reshape(3,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "b = np.arange(3)\n",
    "A + b[:, np.newaxis]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which you have otherwise implemented as an in-place for-loop:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for j in range(A.shape[1]):\n",
    "    A[:,j] += b\n",
    "print(A)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Numpy for Matlab users\n",
    "\n",
    "If you know Matlab already, make sure to look at the cheatsheet [Numpy for matlab users](http://mathesaurus.sourceforge.net/matlab-numpy.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Numpy I/O\n",
    "\n",
    "\n",
    "You can import and export data from a variety of formats. For example, you can save a numpy array to a text format using [np.savetxt](http://docs.scipy.org/doc/numpy/reference/generated/numpy.savetxt.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X = np.random.normal(size=(5,5))\n",
    "np.savetxt('data.txt', X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%cat data.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X = np.loadtxt('data.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unless you want your data files to be human readable in text form, it is usually preferable to use [http://docs.scipy.org/doc/numpy/reference/generated/numpy.save.html](http://docs.scipy.org/doc/numpy/reference/generated/numpy.save.html) rather than `np.savetxt`. The array will then be saved in a more compact Numpy (`.npy`) format."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dealing with matfiles\n",
    "\n",
    "If you can export Matlab matrices in versions v4 (Level 1.0), v6 or v7 to 7.2, then you would be able to use [scipy.io.loadmat](http://docs.scipy.org/doc/scipy/reference/generated/scipy.io.loadmat.html) to load your data in numpy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from scipy.io import loadmat, savemat\n",
    "savemat('test.mat', {'X': X})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(loadmat('test.mat'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sampling\n",
    "\n",
    "You implemented a generative model. You want to use [EM](https://en.wikipedia.org/wiki/Expectation%E2%80%93maximization_algorithm) or [MCMC](https://en.wikipedia.org/wiki/Markov_chain_Monte_Carlo). How do you sample ? \n",
    "\n",
    "You first have a look at: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from IPython.display import IFrame\n",
    "IFrame('http://docs.scipy.org/doc/numpy/reference/routines.random.html', width=700, height=700)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sampling categorical data with a specified CDF\n",
    "\n",
    "One option is to use [numpy.random.choice](http://docs.scipy.org/doc/numpy/reference/generated/numpy.random.choice.html#numpy.random.choice) with the `p` argument:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "probs = [0.75, 0.05, 0.2]\n",
    "samples = np.random.choice([0,1,2], size=1000, p=probs)\n",
    "np.sum(samples == 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or to implement the [inverse sampling method](https://en.wikipedia.org/wiki/Inverse_transform_sampling) yourself:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "samples = np.argmax(np.greater(np.cumsum(probs), np.random.rand(1000)[:, np.newaxis]), axis=1) \n",
    "np.sum(samples == 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reproducibility with randomness\n",
    "\n",
    "In order to get reproducible results with probabilistic algorithms, it is often useful to fix the [random seed](https://en.wikipedia.org/wiki/Random_seed) for the pseudo-random number generator. \n",
    "\n",
    "In Numpy, you construct a [`numpy.random.RandomState`](http://docs.scipy.org/doc/numpy/reference/generated/numpy.random.RandomState.html) object with a fixed seed value. If the argument is `None`, the seed is taken from `/dev/urandom/` you get no reproducibility. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rng = np.random.RandomState(seed=1234)\n",
    "np.sum(rng.choice([0,1,2], size=1000, p=probs) == 0.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scikit-learn\n",
    "\n",
    "Scikit-learn offers a variety of classification, regression and clustering algorithms under a unified interface. If you decided to install Anaconda, it should already be available. [Standard datasets](http://scikit-learn.org/stable/datasets/) are shipped with Scikit. They can be quite useful for debugging your algorithm. We will load the *iris* dataset with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn import linear_model, datasets\n",
    "\n",
    "iris = datasets.load_iris()\n",
    "X = iris.data[:,:]\n",
    "Y = iris.target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scikit-learn provides different [cross-validation](http://scikit-learn.org/stable/modules/cross_validation.html) techniques. In the following `StratifiedKFold` is an iterator which outputs pairs of indices for the train and test instances. You then use these indices to slice your dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.cross_validation import StratifiedKFold\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "skf = StratifiedKFold(Y, 3)\n",
    "logreg = linear_model.LogisticRegression(C=1e5)\n",
    "\n",
    "for train, test in skf:\n",
    "    ypred = logreg.fit(X[train], Y[train]).predict(X[test])\n",
    "    print(mean_squared_error(Y[test], ypred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above loop can also be written more succinctly using [cross_val_score](http://scikit-learn.org/stable/modules/cross_validation.html#computing-cross-validated-metrics). By specifying `n_jobs=k`, you can compute the cross-validation loop in parallel in `k` processes. If `k=-1`, all CPUs are used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.cross_validation import cross_val_score\n",
    "cross_val_score(logreg, X, Y, cv=skf, scoring='mean_squared_error', n_jobs=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model selection\n",
    "\n",
    "Using cross-validated scores, you can perform model selection efficiently through [GridSearchCV](http://scikit-learn.org/stable/modules/generated/sklearn.grid_search.GridSearchCV.html#sklearn.grid_search.GridSearchCV). In the following, we will load the digits dataset and perform grid search over the parameters of an SVM classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "digits = datasets.load_digits()\n",
    "n_samples = len(digits.images)\n",
    "X = digits.images.reshape((n_samples, -1))\n",
    "y = digits.target\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.5, random_state=0)\n",
    "\n",
    "tuned_parameters = [{'kernel': ['rbf'], 'gamma': [1e-3, 1e-4],\n",
    "                     'C': [1, 10, 100, 1000]},\n",
    "                    {'kernel': ['linear'], 'C': [1, 10, 100, 1000]}]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`GridSearchCV` takes an estimator (regression, classification or clustering), a dictionary of parameters to try for that particular estimator, a cross-validation iterator and a scoring function. Once again, the parameter search can be performed in parallel with `n_jobs=-1`. The `fit` function of `GridSearchCV` then scores every possible combination of the `tuned_parameters` and saves the best one in the `best_estimator_` attribute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "skf = StratifiedKFold(Y, 5)\n",
    "clf = GridSearchCV(SVC(C=1), tuned_parameters, cv=skf, n_jobs=-1)\n",
    "clf.fit(X_train, y_train)\n",
    "print(clf.best_estimator_)\n",
    "print(\"Grid scores on train set:\")\n",
    "for params, mean_score, scores in clf.grid_scores_:\n",
    "    print(\"%0.3f (+/-%0.03f) for %r\"%(mean_score, scores.std() / 2, params))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The classification performance of the best estimator can then be reported with [classification_report](http://scikit-learn.org/stable/modules/generated/sklearn.metrics.classification_report.html#sklearn.metrics.classification_report)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "y_true, y_pred = y_test, clf.predict(X_test)\n",
    "print(classification_report(y_true, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dealing with large parameter sets\n",
    "\n",
    "It might happen that the set of parameters over which you would hope to run `GridSearchCV` would be too large. We could then decide to put a prior or the parameters and perform a stochastic search. [RandomizedSearchCV](http://scikit-learn.org/stable/modules/generated/sklearn.grid_search.RandomizedSearchCV.html#sklearn.grid_search.RandomizedSearchCV) supports any of the standard distribution from [scipy.stats](http://docs.scipy.org/doc/scipy/reference/stats.html) as priors. In this example, we will set an exponential priors over the `C` and `gamma` parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import scipy.stats\n",
    "from sklearn.grid_search import RandomizedSearchCV\n",
    "\n",
    "tuned_parameters = {'C': scipy.stats.expon(scale=100), 'gamma': scipy.stats.expon(scale=.1),\n",
    "  'kernel': ['rbf'], 'class_weight':['auto', None]}\n",
    "\n",
    "clf = RandomizedSearchCV(SVC(C=1), tuned_parameters, n_iter=10, n_jobs=-1)\n",
    "clf.fit(X_train, y_train)\n",
    "print(clf.best_estimator_)\n",
    "y_true, y_pred = y_test, clf.predict(X_test)\n",
    "print(classification_report(y_true, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fancier model selection\n",
    "\n",
    "[Hyperopt](https://github.com/hyperopt/hyperopt) goes beyond the randomized and grid search methods and provides fancier *hyperparameters* optimization. This package is quite popular in the neural net communities. \n",
    "\n",
    "It has also been *sklearnified* under [hyperopt-sklearn](https://github.com/hyperopt/hyperopt-sklearn)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plotting the confusion matrix\n",
    "\n",
    "The confusion matrix is a useful visual tool to asess the classification performance. In the ideal case, you would see only entries on the diagonal. In this case, every label would have been predicted perfectly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "plt.matshow(cm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generating an ROC curve"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wee can compute the Receiver Operating Characteristic (ROC) curve to assess the quality of a binary classifier. In the following, we will use a synthetic binary classification task from the Hastie textbook. It consists of 12000 instances, each with 10 Gaussian features. We will make use of the probabilistic interpretation of logistic regression in order to produce the ROC curve. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_hastie_10_2 \n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "X, y = make_hastie_10_2()\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.5, random_state=0)\n",
    "\n",
    "tuned_parameters = [{'penalty': ['l1', 'l2'],\n",
    "                     'C': [1, 10, 100, 1000]}]\n",
    "\n",
    "logreg = LogisticRegression()\n",
    "clf = GridSearchCV(logreg, tuned_parameters)\n",
    "clf.fit(X_train, y_train)\n",
    "clf.best_score_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The [roc_curve](http://scikit-learn.org/stable/modules/generated/sklearn.metrics.roc_curve.html) function takes as inputs the true labels for the binary classification as well as the probability estimate of the dominant class. We use `np.max` with `axis=1` to compute the max across the columns (the second axis)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve\n",
    "y_pred = clf.predict_proba(X_test)\n",
    "y_score = np.max(y_pred, axis=1)\n",
    "fpr, tpr, th = roc_curve(y_test, y_score)\n",
    "plt.plot([0, 1], [0, 1], 'k--')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.plot(fpr, tpr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a pretty bad ROC curve."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plotting the learning curve \n",
    "\n",
    "[sklearn.learning_curve.learning_curve](http://scikit-learn.org/stable/modules/generated/sklearn.learning_curve.learning_curve.html#sklearn.learning_curve) is another one-liner which might be useful for illustrating the learning behavior of a given a model (for a fixed choice of parameters). Building upon our last example, we will plot the learning curve of the best model that we found through grid search. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.learning_curve import learning_curve\n",
    "from sklearn.cross_validation import ShuffleSplit\n",
    "\n",
    "cv = ShuffleSplit(digits.data.shape[0], n_iter=10,\n",
    "                                   test_size=0.2, random_state=0)\n",
    "\n",
    "train_sizes, train_scores, test_scores = learning_curve(clf.best_estimator_, X, y, cv=cv,\n",
    "                                                        train_sizes=np.linspace(.1, 1.0, 5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use the [fill_between](http://matplotlib.org/examples/pylab_examples/fill_between_demo.html) plotting function of Matplotlib to show the standard deviation in the score estimates. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_scores_mean = np.mean(train_scores, axis=1)\n",
    "train_scores_std = np.std(train_scores, axis=1)\n",
    "test_scores_mean = np.mean(test_scores, axis=1)\n",
    "test_scores_std = np.std(test_scores, axis=1)\n",
    "plt.grid()\n",
    "plt.fill_between(train_sizes, train_scores_mean - train_scores_std,\n",
    "                     train_scores_mean + train_scores_std, alpha=0.1,\n",
    "                     color=\"r\")\n",
    "plt.fill_between(train_sizes, test_scores_mean - test_scores_std,\n",
    "                     test_scores_mean + test_scores_std, alpha=0.1, color=\"g\")\n",
    "plt.plot(train_sizes, train_scores_mean, 'o-', color=\"r\",\n",
    "             label=\"Training score\")\n",
    "plt.plot(train_sizes, test_scores_mean, 'o-', color=\"g\",\n",
    "             label=\"Cross-validation score\")\n",
    "\n",
    "plt.legend(loc=\"best\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# One-hot encoding of categorical features\n",
    "\n",
    "Scikit-learn also offers convenient preprocessing functions for the most common cases. Let say that you have categorial data of the following kind:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "measurements = [\n",
    "     {'month': 'January', 'temperature': 33.},\n",
    "     {'month': 'March', 'temperature': 12.},\n",
    "     {'month': 'May', 'temperature': 18.},\n",
    "     {'month': 'January', 'temperature': 12.},\n",
    " ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[DictVectorizer](http://scikit-learn.org/stable/modules/feature_extraction.html#loading-features-from-dicts) can be used to transform lists of dictionaries. Categorial variables are transformed using one-hot encoding. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction import DictVectorizer\n",
    "vec = DictVectorizer()\n",
    "vec.fit_transform(measurements).toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Counting word frequencies\n",
    "\n",
    "When working with text data, you often have to estimate empirical probabilities. [CountVectorizer](http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html) allows you to write a one-liner for this task. We will use the [20 Newsgroup](http://qwone.com/~jason/20Newsgroups/) dataset to illustate this usecase. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "twenty_train = fetch_20newsgroups()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "X_train_counts = vectorizer.fit_transform(twenty_train.data)\n",
    "vectorizer.vocabulary_.get(u'algorithm')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By default, `CountVectorizer` only extracts frequencies for single tokens. When looking at pair or triples of tokens, we then talk about n-grams models. `CountVectorizer` takes a `ngram_range` argument specifying the minimum and maximum order of the n-grams to extract. \n",
    "\n",
    "The scikit-learn documentation has a [dedicated page](http://scikit-learn.org/stable/tutorial/text_analytics/working_with_text_data.html) on text data. Make sure to take a look !\n",
    "\n",
    "Oh, and here's a fun exercise ! Using `CountVectorizer`, estimate the empirical probabilies of some text corpus and sample sequences of words starting from an initial word. This would give you a primitive Markov Chain approach to text generation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learning more\n",
    "\n",
    "* [NLTK](http://www.nltk.org/) for more advanced Natural Language Processing (NLP) tools in Python\n",
    "* [statsmodels](http://statsmodels.sourceforge.net/) for autoregressive models\n",
    "* [Pandas](http://pandas.pydata.org/) for data gymnastics on structured datasets\n",
    "* [matplotlib](http://matplotlib.org/gallery.html) to learn how to generate fancy plots\n",
    "* [networkx](https://networkx.github.io/) for representing and operating on graphs\n",
    "* [Starcluster, Statsmodels and Pandas](http://nbviewer.ipython.org/github/pierrelux/notebooks/blob/master/Starcluster%20and%20IPython%20tutorial.ipynb) another tutorial that I wrote on using Starcluster on Amazon EC2 in combination with statsmodels and Pandas\n",
    "* [Diving into Open Data with IPython Notebook & Pandas](http://nbviewer.ipython.org/github/jvns/talks/blob/master/pyconca2013/pistes-cyclables.ipynb) on using Pandas on the Bixi dataset. (Btw, Julia is a former student of Prakash).\n",
    "* [How to convert notebooks to Latex](http://ipython.org/ipython-doc/1/interactive/nbconvert.html) A great way to submit your assignments !"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
